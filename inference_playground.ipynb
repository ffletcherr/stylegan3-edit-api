{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "# Third Time's the Charm? StyleGAN3 Inference Notebook"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "## Prepare Environment and Download Code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "#@title Clone Repo and Install Ninja { display-mode: \"form\" }\n",
    "import os\n",
    "from pathlib import Path\n",
    "\n",
    "os.chdir('/content')\n",
    "CODE_DIR = 'stylegan3-editing'\n",
    "\n",
    "## clone repo\n",
    "!git clone https://github.com/ffletcherr/stylegan3-edit-api.git $CODE_DIR\n",
    "\n",
    "## install ninja\n",
    "!wget https://github.com/ninja-build/ninja/releases/download/v1.8.2/ninja-linux.zip\n",
    "!sudo unzip ninja-linux.zip -d /usr/local/bin/\n",
    "!sudo update-alternatives --install /usr/bin/ninja ninja /usr/local/bin/ninja 1 --force\n",
    "\n",
    "## install some packages\n",
    "!pip install pyrallis\n",
    "!pip install git+https://github.com/openai/CLIP.git\n",
    "\n",
    "os.chdir(f'./{CODE_DIR}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "#@title Import Packages { display-mode: \"form\" }\n",
    "import time\n",
    "import sys\n",
    "import pprint\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "import dataclasses\n",
    "import torch\n",
    "import torchvision.transforms as transforms\n",
    "\n",
    "\n",
    "from editing.interfacegan.face_editor import FaceEditor\n",
    "from editing.styleclip_global_directions import edit as styleclip_edit\n",
    "from models.stylegan3.model import GeneratorType\n",
    "from utils.common import tensor2im\n",
    "from utils.inference_utils import run_on_batch, load_encoder, get_average_image\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from oauth2client.client import GoogleCredentials\n",
    "import dlib\n",
    "import subprocess\n",
    "\n",
    "from utils.alignment_utils import align_face, crop_face, get_stylegan_transform\n",
    "\n",
    "\n",
    "ENCODER_PATHS = {\n",
    "    \"restyle_e4e_ffhq\": {\"id\": \"1z_cB187QOc6aqVBdLvYvBjoc93-_EuRm\", \"name\": \"restyle_e4e_ffhq.pt\"},\n",
    "    \"restyle_pSp_ffhq\": {\"id\": \"12WZi2a9ORVg-j6d9x4eF-CKpLaURC2W-\", \"name\": \"restyle_pSp_ffhq.pt\"},\n",
    "}\n",
    "INTERFACEGAN_PATHS = {\n",
    "    \"age\": {'id': '1NQVOpKX6YZKVbz99sg94HiziLXHMUbFS', 'name': 'age_boundary.npy'},\n",
    "    \"smile\": {'id': '1KgfJleIjrKDgdBTN4vAz0XlgSaa9I99R', 'name': 'Smiling_boundary.npy'},\n",
    "    \"pose\": {'id': '1nCzCR17uaMFhAjcg6kFyKnCCxAKOCT2d', 'name': 'pose_boundary.npy'},\n",
    "    \"Male\": {'id': '18dpXS5j1h54Y3ah5HaUpT03y58Ze2YEY', 'name': 'Male_boundary.npy'}\n",
    "}\n",
    "STYLECLIP_PATHS = {\n",
    "    \"delta_i_c\": {\"id\": \"1HOUGvtumLFwjbwOZrTbIloAwBBzs2NBN\", \"name\": \"delta_i_c.npy\"},\n",
    "    \"s_stats\": {\"id\": \"1FVm_Eh7qmlykpnSBN1Iy533e_A2xM78z\", \"name\": \"s_stats\"},\n",
    "}\n",
    "\n",
    "\n",
    "class Downloader:\n",
    "\n",
    "    def __init__(self, code_dir, use_pydrive, subdir):\n",
    "        self.use_pydrive = use_pydrive\n",
    "        current_directory = os.getcwd()\n",
    "        self.save_dir = os.path.join(os.path.dirname(current_directory), code_dir, subdir)\n",
    "        os.makedirs(self.save_dir, exist_ok=True)\n",
    "\n",
    "    def download_file(self, file_id, file_name):\n",
    "        file_dst = f'{self.save_dir}/{file_name}'\n",
    "        if os.path.exists(file_dst):\n",
    "            print(f'{file_name} already exists!')\n",
    "            return\n",
    "        if self.use_pydrive:\n",
    "            downloaded = self.drive.CreateFile({'id': file_id})\n",
    "            downloaded.FetchMetadata(fetch_all=True)\n",
    "            downloaded.GetContentFile(file_dst)\n",
    "        else:\n",
    "            command = self._get_download_model_command(file_id=file_id, file_name=file_name)\n",
    "            subprocess.run(command, shell=True, stdout=subprocess.PIPE)\n",
    "\n",
    "    def _get_download_model_command(self, file_id, file_name):\n",
    "        \"\"\" Get wget download command for downloading the desired model and save to directory ../pretrained_models. \"\"\"\n",
    "        url = r\"\"\"wget --load-cookies /tmp/cookies.txt \"https://docs.google.com/uc?export=download&confirm=$(wget --quiet --save-cookies /tmp/cookies.txt --keep-session-cookies --no-check-certificate 'https://docs.google.com/uc?export=download&id={FILE_ID}' -O- | sed -rn 's/.*confirm=([0-9A-Za-z_]+).*/\\1\\n/p')&id={FILE_ID}\" -O {SAVE_PATH}/{FILE_NAME} && rm -rf /tmp/cookies.txt\"\"\".format(FILE_ID=file_id, FILE_NAME=file_name, SAVE_PATH=self.save_dir)\n",
    "        return url\n",
    "\n",
    "\n",
    "def download_dlib_models():\n",
    "    if not os.path.exists(\"shape_predictor_68_face_landmarks.dat\"):\n",
    "        print('Downloading files for aligning face image...')\n",
    "        os.system('wget http://dlib.net/files/shape_predictor_68_face_landmarks.dat.bz2')\n",
    "        os.system('bzip2 -dk shape_predictor_68_face_landmarks.dat.bz2')\n",
    "        print('Done.')\n",
    "\n",
    "\n",
    "def run_alignment(image_path):\n",
    "    download_dlib_models()\n",
    "    predictor = dlib.shape_predictor(\"shape_predictor_68_face_landmarks.dat\")\n",
    "    detector = dlib.get_frontal_face_detector()\n",
    "    print(\"Aligning image...\")\n",
    "    aligned_image = align_face(filepath=str(image_path), detector=detector, predictor=predictor)\n",
    "    print(f\"Finished aligning image: {image_path}\")\n",
    "    return aligned_image\n",
    "\n",
    "\n",
    "def crop_image(image_path):\n",
    "    download_dlib_models()\n",
    "    predictor = dlib.shape_predictor(\"shape_predictor_68_face_landmarks.dat\")\n",
    "    detector = dlib.get_frontal_face_detector()\n",
    "    print(\"Cropping image...\")\n",
    "    cropped_image = crop_face(filepath=str(image_path), detector=detector, predictor=predictor)\n",
    "    print(f\"Finished cropping image: {image_path}\")\n",
    "    return cropped_image\n",
    "\n",
    "\n",
    "def compute_transforms(aligned_path, cropped_path):\n",
    "    download_dlib_models()\n",
    "    predictor = dlib.shape_predictor(\"shape_predictor_68_face_landmarks.dat\")\n",
    "    detector = dlib.get_frontal_face_detector()\n",
    "    print(\"Computing landmarks-based transforms...\")\n",
    "    res = get_stylegan_transform(str(cropped_path), str(aligned_path), detector, predictor)\n",
    "    print(\"Done!\")\n",
    "    if res is None:\n",
    "        print(f\"Failed computing transforms on: {cropped_path}\")\n",
    "        return\n",
    "    else:\n",
    "        rotation_angle, translation, transform, inverse_transform = res\n",
    "        return inverse_transform\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "## Define Download Configuration\n",
    "Select below whether you wish to download all models using `pydrive`. Note that if you do not use `pydrive`, you may encounter a \"quota exceeded\" error from Google Drive."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "#@title { display-mode: \"form\" }\n",
    "download_with_pydrive = False #@param {type:\"boolean\"}\n",
    "downloader = Downloader(code_dir=CODE_DIR,\n",
    "                        use_pydrive=download_with_pydrive,\n",
    "                        subdir=\"pretrained_models\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "## Select Model for Inference\n",
    "Currently, we have ReStyle-pSp and ReStyle-e4e encoders trained for human faces."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "#@title Select which model/domain you wish to perform inference on: { display-mode: \"form\" }\n",
    "experiment_type = 'restyle_pSp_ffhq' #@param ['restyle_e4e_ffhq', 'restyle_pSp_ffhq']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "## Define Inference Parameters\n",
    "\n",
    "Below we have a dictionary defining parameters such as the path to the pretrained model to use and the path to the image to perform inference on. While we provide default values to run this script, feel free to change as needed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "EXPERIMENT_DATA_ARGS = {\n",
    "    \"restyle_pSp_ffhq\": {\n",
    "        \"model_path\": \"./pretrained_models/restyle_pSp_ffhq.pt\",\n",
    "        \"image_path\": \"./images/face_image.jpg\",\n",
    "        \"transform\": transforms.Compose([\n",
    "            transforms.Resize((256, 256)),\n",
    "            transforms.ToTensor(),\n",
    "            transforms.Normalize([0.5, 0.5, 0.5], [0.5, 0.5, 0.5])])\n",
    "    },\n",
    "    \"restyle_e4e_ffhq\": {\n",
    "        \"model_path\": \"./pretrained_models/restyle_e4e_ffhq.pt\",\n",
    "        \"image_path\": \"./images/face_image.jpg\",\n",
    "        \"transform\": transforms.Compose([\n",
    "            transforms.Resize((256, 256)),\n",
    "            transforms.ToTensor(),\n",
    "            transforms.Normalize([0.5, 0.5, 0.5], [0.5, 0.5, 0.5])])\n",
    "    }\n",
    "}\n",
    "\n",
    "EXPERIMENT_ARGS = EXPERIMENT_DATA_ARGS[experiment_type]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "## Download Models\n",
    "To reduce the number of requests to fetch the model, we'll check if the model was previously downloaded and saved before downloading the model.\n",
    "We'll download the model for the selected experiment and save it to the folder `stylegan3-editing/pretrained_models`.\n",
    "\n",
    "We also need to verify that the model was downloaded correctly.\n",
    "Note that if the file weighs several KBs, you most likely encounter a \"quota exceeded\" error from Google Drive."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "#@title Download ReStyle SG3 Encoder { display-mode: \"form\" }\n",
    "if not os.path.exists(EXPERIMENT_ARGS['model_path']) or os.path.getsize(EXPERIMENT_ARGS['model_path']) < 1000000:\n",
    "    print(f'Downloading ReStyle encoder model: {experiment_type}...')\n",
    "    try:\n",
    "      downloader.download_file(file_id=ENCODER_PATHS[experiment_type]['id'],\n",
    "                              file_name=ENCODER_PATHS[experiment_type]['name'])\n",
    "    except Exception as e:\n",
    "      raise ValueError(f\"Unable to download model correctly! {e}\")\n",
    "    # if google drive receives too many requests, we'll reach the quota limit and be unable to download the model\n",
    "    if os.path.getsize(EXPERIMENT_ARGS['model_path']) < 1000000:\n",
    "        raise ValueError(\"Pretrained model was unable to be downloaded correctly!\")\n",
    "    else:\n",
    "        print('Done.')\n",
    "else:\n",
    "    print(f'Model for {experiment_type} already exists!')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "## Load Pretrained Model\n",
    "We assume that you have downloaded all relevant models and placed them in the directory defined by the\n",
    "`EXPERIMENT_DATA_ARGS` dictionary."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "#@title Load ReStyle SG3 Encoder { display-mode: \"form\" }\n",
    "model_path = EXPERIMENT_ARGS['model_path']\n",
    "net, opts = load_encoder(checkpoint_path=model_path)\n",
    "pprint.pprint(dataclasses.asdict(opts))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "## Prepare Inputs\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "#@title Define and Visualize Input { display-mode: \"form\" }\n",
    "\n",
    "image_path = Path(EXPERIMENT_DATA_ARGS[experiment_type][\"image_path\"])\n",
    "original_image = Image.open(image_path).convert(\"RGB\")\n",
    "original_image = original_image.resize((256, 256))\n",
    "original_image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "#@title Get Aligned and Cropped Input Images\n",
    "input_image = run_alignment(image_path)\n",
    "cropped_image = crop_image(image_path)\n",
    "joined = np.concatenate([input_image.resize((256, 256)), cropped_image.resize((256, 256))], axis=1)\n",
    "Image.fromarray(joined)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "#@title Compute Landmarks-Based Transforms { display-mode: \"form\" }\n",
    "images_dir = Path(\"./images\")\n",
    "images_dir.mkdir(exist_ok=True, parents=True)\n",
    "cropped_path = images_dir / f\"cropped_{image_path.name}\"\n",
    "aligned_path = images_dir / f\"aligned_{image_path.name}\"\n",
    "cropped_image.save(cropped_path)\n",
    "input_image.save(aligned_path)\n",
    "landmarks_transform = compute_transforms(aligned_path=aligned_path, cropped_path=cropped_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "## Perform Inversion\n",
    "Now we'll run inference. By default, we'll run using 3 inference steps. You can change the parameter in the cell below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "#@title { display-mode: \"form\" }\n",
    "n_iters_per_batch = 3 #@param {type:\"integer\"}\n",
    "opts.n_iters_per_batch = n_iters_per_batch\n",
    "opts.resize_outputs = False  # generate outputs at full resolution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "#@title Run Inference { display-mode: \"form\" }\n",
    "img_transforms = EXPERIMENT_ARGS['transform']\n",
    "transformed_image = img_transforms(input_image)\n",
    "\n",
    "avg_image = get_average_image(net)\n",
    "\n",
    "with torch.no_grad():\n",
    "    tic = time.time()\n",
    "    result_batch, result_latents = run_on_batch(inputs=transformed_image.unsqueeze(0).cuda().float(),\n",
    "                                                net=net,\n",
    "                                                opts=opts,\n",
    "                                                avg_image=avg_image,\n",
    "                                                landmarks_transform=torch.from_numpy(landmarks_transform).cuda().float())\n",
    "    toc = time.time()\n",
    "    print('Inference took {:.4f} seconds.'.format(toc - tic))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "#@title Visualize Result { display-mode: \"form\" }\n",
    "\n",
    "def get_coupled_results(result_batch, cropped_image):\n",
    "    result_tensors = result_batch[0]  # there's one image in our batch\n",
    "    resize_amount = (256, 256) if opts.resize_outputs else (opts.output_size, opts.output_size)\n",
    "    final_rec = tensor2im(result_tensors[-1]).resize(resize_amount)\n",
    "    input_im = cropped_image.resize(resize_amount)\n",
    "    res = np.concatenate([np.array(input_im), np.array(final_rec)], axis=1)\n",
    "    res = Image.fromarray(res)\n",
    "    return res\n",
    "\n",
    "res = get_coupled_results(result_batch, cropped_image)\n",
    "res.resize((1024, 512))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "#@title Save Result { display-mode: \"form\" }\n",
    "\n",
    "# save image\n",
    "outputs_path = \"./outputs\"\n",
    "os.makedirs(outputs_path, exist_ok=True)\n",
    "res.save(os.path.join(outputs_path, os.path.basename(image_path)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "# Editing\n",
    "Given the resulting latent code obtained above, we can perform various edits, which we demonstrate below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "#@title Download pretrained boundaries and editing files: { display-mode: \"form\" }\n",
    "download_with_pydrive = True #@param {type:\"boolean\"}\n",
    "\n",
    "# download files for interfacegan\n",
    "downloader = Downloader(code_dir=CODE_DIR,\n",
    "                        use_pydrive=download_with_pydrive,\n",
    "                        subdir=\"editing/interfacegan/boundaries/ffhq\")\n",
    "print(\"Downloading InterFaceGAN boundaries...\")\n",
    "for editing_file, params in INTERFACEGAN_PATHS.items():\n",
    "    print(f\"Downloading {editing_file} boundary...\")\n",
    "    downloader.download_file(file_id=params['id'],\n",
    "                             file_name=params['name'])\n",
    "\n",
    "# download files for styleclip\n",
    "downloader = Downloader(code_dir=CODE_DIR,\n",
    "                        use_pydrive=download_with_pydrive,\n",
    "                        subdir=\"editing/styleclip_global_directions/sg3-r-ffhq-1024\")\n",
    "print(\"Downloading StyleCLIP auxiliary files...\")\n",
    "for editing_file, params in STYLECLIP_PATHS.items():\n",
    "    print(f\"Downloading {editing_file}...\")\n",
    "    downloader.download_file(file_id=params['id'],\n",
    "                             file_name=params['name'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "### InterFaceGAN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "editor = FaceEditor(stylegan_generator=net.decoder, generator_type=GeneratorType.ALIGNED)\n",
    "\n",
    "#@title Select which edit you wish to perform: { display-mode: \"form\" }\n",
    "edit_direction = 'age' #@param ['age', 'smile', 'pose', 'Male']\n",
    "min_value = -5 #@param {type:\"slider\", min:-10, max:10, step:1}\n",
    "max_value = 5 #@param {type:\"slider\", min:-10, max:10, step:1}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "#@title Perform Edit! { display-mode: \"form\" }\n",
    "print(f\"Performing edit for {edit_direction}...\")\n",
    "input_latent = torch.from_numpy(result_latents[0][-1]).unsqueeze(0).cuda()\n",
    "edit_images, edit_latents = editor.edit(latents=input_latent,\n",
    "                                        direction=edit_direction,\n",
    "                                        factor_range=(min_value, max_value),\n",
    "                                        user_transforms=landmarks_transform,\n",
    "                                        apply_user_transformations=True)\n",
    "print(\"Done!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "#@title Show Result { display-mode: \"form\" }\n",
    "def prepare_edited_result(edit_images):\n",
    "  if type(edit_images[0]) == list:\n",
    "      edit_images = [image[0] for image in edit_images]\n",
    "  res = np.array(edit_images[0].resize((512, 512)))\n",
    "  for image in edit_images[1:]:\n",
    "      res = np.concatenate([res, image.resize((512, 512))], axis=1)\n",
    "  res = Image.fromarray(res).convert(\"RGB\")\n",
    "  return res\n",
    "\n",
    "res = prepare_edited_result(edit_images)\n",
    "res"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "### StyleCLIP (Global Directions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "#@title Prepare StyleCLIP Editor { display-mode: \"form\" }\n",
    "styleclip_args = styleclip_edit.EditConfig()\n",
    "global_direction_calculator = styleclip_edit.load_direction_calculator(stylegan_model=net.decoder, opts=styleclip_args)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "#@title Select which edit you wish to perform. { display-mode: \"form\" }\n",
    "#@markdown We recommend keeping the neutral_text as is by default.\n",
    "neutral_text = \"a face\" #@param {type:\"raw\"}\n",
    "target_text = \"a smiling face\" #@param {type:\"raw\"}\n",
    "alpha = 4 #@param {type:\"slider\", min:-5, max:5, step:0.5}\n",
    "beta = 0.13 #@param {type:\"slider\", min:-1, max:1, step:0.1}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "#@title Perform Edit! { display-mode: \"form\" }\n",
    "opts = styleclip_edit.EditConfig()\n",
    "opts.alpha_min = alpha\n",
    "opts.alpha_max = alpha\n",
    "opts.num_alphas = 1\n",
    "opts.beta_min = beta\n",
    "opts.beta_max = beta\n",
    "opts.num_betas = 1\n",
    "opts.neutral_text = neutral_text\n",
    "opts.target_text = target_text\n",
    "\n",
    "input_latent = result_latents[0][-1]\n",
    "input_transforms = torch.from_numpy(landmarks_transform).cpu().numpy()\n",
    "print(f'Performing edit for: \"{opts.target_text}\"...')\n",
    "edit_res, edit_latent = styleclip_edit.edit_image(latent=input_latent,\n",
    "                                                  landmarks_transform=input_transforms,\n",
    "                                                  stylegan_model=net.decoder,\n",
    "                                                  global_direction_calculator=global_direction_calculator,\n",
    "                                                  opts=opts,\n",
    "                                                  image_name=None,\n",
    "                                                  save=False)\n",
    "print(\"Done!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "#@title Show Result { display-mode: \"form\" }\n",
    "input_im = tensor2im(transformed_image).resize((512, 512))\n",
    "edited_im = tensor2im(edit_res[0]).resize((512, 512))\n",
    "edit_coupled = np.concatenate([np.array(input_im), np.array(edited_im)], axis=1)\n",
    "edit_coupled = Image.fromarray(edit_coupled)\n",
    "edit_coupled.resize((1024, 512))"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "inference_playground (5).ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
